# JS 散度的两种书写方式

2023/10/23 - april211

## 导引

最近在读有关半监督学习的一篇综述文章[^1]，文章中有协同训练（co-training）相关的内容，其中提到了与这份工作[^2]相关的内容。

这份工作中使用到了信息论中与熵（Entropy）相关的概念，还使用了 JS 散度来构造损失函数。**但是文章中所给出的 JS 散度的公式是直接使用熵来定义的，而不是常见的使用 KL 散度来定义的形式。所以这里记录一下两者的等价关系。**

推导过程实际上很简单，但是这种拆分重组的 trick 在最近遇到的公式推导中多有涉及，而许多参考资料上给出的定义[^3] [^4] [^5]大多是基于 KL 散度的定义，所以作为一名数学菜鸡，感觉有必要在这里做一下记录 😂

**PS**: 本文不会对熵相关的概念进行详细的阐述，想先大致了解一下的同学，可以参考下邱锡鹏老师的这本书[^5]的附录中有关信息论的介绍。

## 等价关系推导

这里参考博客[^3]，给出基于 KL 散度的的 JS 散度的定义如 ([1](#mjx-eqn-eq1)) 式所示。

$` \begin{equation} \label{eq1}\tag{1} JSD(P_1||P_2) = \frac{1}{2} KL(P_1||M) + \frac{1}{2} KL(P_2||M) \end{equation} `$ 

其中，$` P_1 `$ 、$` P_2 `$ 分别表示两个不同的概率分布，$` M= \frac{P_1(x) + P_2(x)}{2} `$ 是两个概率分布的平均值，$`  KL(A||B)  `$ 值 KL 散度，其公式如 ([2](#mjx-eqn-eq2)) 式、([3](#mjx-eqn-eq3)) 式所示。

$` \begin{equation} \label{eq2}\tag{2} KL(P_1||P_2) = \sum_{x} p_1(x)\frac{p_1(x)}{p_2(x)} \end{equation} `$ 

$` \begin{equation} \label{eq3}\tag{3} KL(P_2||P_1) = \sum_{x} p_2(x)\frac{p_2(x)}{p_1(x)} \end{equation} `$ 

*BTW，KL 散度是不具备对称性的，这也是 JS 散度对它做出的改进方向。*

而论文[^2]中使用的 JS 散度的表达式如 ([4](#mjx-eqn-eq4)) 式所示。

$` \begin{equation} \label{eq4}\tag{4} JSD(P_1||P_2) = H(\frac{1}{2}(p_1(x)+p_2(x))) - \frac{1}{2}(H(p_1(x))+H(p_2(x))) \end{equation} `$ 

其中

$` \begin{equation} \label{eq5}\tag{5} H(p(x)) = -\sum_{x} p(x)logp(x) \end{equation}`$

表示随机变量 $` X `$ 的熵（自信息的期望值）。

乍一看，这两个公式在形式上好像很不一样。但是如果我们进行如下的推导，就能发现 ([1](#mjx-eqn-eq1)) 式和 ([4](#mjx-eqn-eq4)) 式其实是**等价**的。

我们将 KL 散度的公式 ([2](#mjx-eqn-eq2)) 、([3](#mjx-eqn-eq3)) 带入到 ([1](#mjx-eqn-eq1)) 式中：

$$
\begin{align}
JSD(P_1||P_2) 
&= \frac{1}{2} KL(P_1||M) + \frac{1}{2} KL(P_2||M) \\
&= \frac{1}{2}\sum_{x} p_1(x)log(\frac{p_1(x)}{\frac{p_1(x) + p_2(x)}{2}}) + \frac{1}{2}\sum_{x} p_2(x)log(\frac{p_2(x)}{\frac{p_1(x) + p_2(x)}{2}}) \\
\end{align}
$$

然后分别拆分两项，得到：

$$
\begin{align}
JSD(P_1||P_2) 
&= \frac{1}{2}\sum_{x} p_1(x)logp_1(x) - \frac{1}{2}\sum_{x} p_1(x)log(\frac{p_1(x) + p_2(x)}{2}) \\ 
&+ \frac{1}{2}\sum_{x} p_2(x)logp_2(x) - \frac{1}{2}\sum_{x} p_2(x)log(\frac{p_1(x) + p_2(x)}{2}) \\
&= - \frac{1}{2}(H(p_1(x))+H(p_2(x))) + H(\frac{1}{2}(p_1(x)+p_2(x))) \\
&= (\ref{eq4})
\end{align}
$$

这里先将 KL 散度的两项进行拆分，从而获得类似熵定义的式子，随后自然就可以想到合并相似的项。

其中，第一列两项的合并是显而易见的，第二列两项的合并过程如下。

$$
\begin{align}
& - \frac{1}{2}\sum_{x} p_1(x)log(\frac{p_1(x) + p_2(x)}{2})  - \frac{1}{2}\sum_{x} p_2(x)log(\frac{p_1(x) + p_2(x)}{2}) \\ 
=& - \sum_{x} \frac{1}{2}p_1(x)log(\frac{p_1(x) + p_2(x)}{2}) + \frac{1}{2}p_2(x)log(\frac{p_1(x) + p_2(x)}{2}) \\ 
=& - \sum_{x} \frac{p_1(x)+p_2(x)}{2}log(\frac{p_1(x) + p_2(x)}{2}) \\ 
=& H(\frac{1}{2}(p_1(x)+p_2(x)))
\end{align}
$$

这样我们便完成了从而完成了 ([1](#mjx-eqn-eq1)) 式与 ([4](#mjx-eqn-eq4)) 式相等的证明。这种 **拆分-合并相似项** 的形式通常可以化简公式、简化计算。

## 题外话

博客[^3]对 JS 散度的缺陷（当两个分布完全不重叠时，不管两个分布的中心距离有多近，其 JS 散度值都是一个常数，以至于其梯度为0）进行了推导，这次的推导整理也受到这篇博客的启发。对此有疑惑的同学可以去围观下～

在博客的证明过程中，可以把 $`log2`$ 取出来是因为概率密度的性质： $` \sum p_1(x) = 1, \sum p_2(x) = 1`$，从而：

$$
\begin{equation}
\begin{aligned}
C &= \frac{1}{2} \sum_{x}p_1(x)log2 + \frac{1}{2} \sum_{x}p_2(x)log2 \\
&= log2(\frac{1}{2} \sum_{x}p_1(x) + \frac{1}{2} \sum_{x}p_2(x)) \\
&= log2 & 
\end{aligned}
\end{equation}
$$


[^1]: YANG X, SONG Z, KING I, et al. A survey on deep semi-supervised learning[J]. IEEE Transactions on Knowledge and Data Engineering, 2022.

[^2]: QIAO S, SHEN W, ZHANG Z, et al. Deep co-training for semi-supervised image recognition[C].Proceedings of the european conference on computer vision (eccv). 2018: 135-152.

[^3]: GAN：两者分布不重合JS散度为log2的数学证明_为什么深度学习wganjs散度等于log2-CSDN博客[EB/OL]. [2023-10-23]. https://blog.csdn.net/Invokar/article/details/88917214.

[^4]: 交叉熵、相对熵（KL散度）、JS散度和Wasserstein距离（推土机距离）[EB/OL]//知乎专栏. [2023-10-23]. https://zhuanlan.zhihu.com/p/74075915.

[^5]: 邱锡鹏. 神经网络与深度学习[M/OL]. 北京: 机械工业出版社, 2020. https://nndl.github.io/.

